{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vect.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iF9gO4PeTz2","executionInfo":{"status":"ok","timestamp":1646513362801,"user_tz":480,"elapsed":24427,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"38550ec1-7e12-4851-8adf-867166ceb653"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pickle\n","import pandas\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n","from sklearn.naive_bayes import GaussianNB\n","from torch.functional import norm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from gensim.parsing.preprocessing import preprocess_string\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["with open('drive/MyDrive/CS247/train_data.pkl', 'rb') as fid:\n","     train_data= pickle.load(fid)\n","\n","with open('drive/MyDrive/CS247/valid_data.pkl', 'rb') as fid:\n","     valid_data= pickle.load(fid)\n","\n","with open('drive/MyDrive/CS247/test_data.pkl', 'rb') as fid:\n","     test_data= pickle.load(fid)\n","\n","with open('drive/MyDrive/CS247/data.pkl', 'rb') as fid:\n","     data= pickle.load(fid)"],"metadata":{"id":"KPu9Df7jesMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## FUNCTIONS\n","\n","## =================== DATA PROCESSING ==================== ##\n","def data_preprocess(comments, MIN_COUNT=1):\n","  sent_ids, sent_wds  = [], []\n","  word_count, word2id, id2word = {}, {}, {}\n","\n","  for doc in comments:\n","      for word in doc:\n","          if word not in word_count:\n","              word_count[word] = 0\n","          word_count[word] += 1\n","\n","  for doc in comments:\n","      sent_id = []\n","      sent_wd = []\n","      for word in doc:\n","          if word_count[word] < MIN_COUNT:\n","              continue\n","          if word not in word2id:\n","              idx = len(id2word)\n","              word2id[word] = idx\n","              id2word[idx]  = word\n","          sent_id += [word2id[word]]\n","          sent_wd += [word]\n","      \n","      sent_ids += [sent_id]\n","      sent_wds += [sent_wd]\n","\n","  return sent_ids, sent_wds, word_count, word2id, id2word\n","\n","def remove_empty_tokenized_values(data):\n","  return data.loc[data[\"comment_tokenize\"].str.len() != 0]\n","\n","def index_of_empty_tokenized_values(data):\n","  return [i for i, val in enumerate(data[\"comment_tokenize\"].tolist()) if len(val) == 0]\n","\n","## =========== GET THE EMBEDDINGS FOR THE WORDS =========== ##\n","def get_emb_avg(tokenized_data, word_embeddings, dim):\n","  avg_emb = []\n","  for doc in tokenized_data:\n","    sum_emb = np.zeros(dim)\n","    for wrd in doc:\n","      curr_emb = word_embeddings[wrd]\n","      sum_emb += curr_emb\n","    avg_emb += [sum_emb / len(doc)]\n","  return avg_emb\n","\n","def weighted_emb(tokenized_data, word_embeddings, avg_emb, dim):\n","  weighted_result = []\n","  print(len(tokenized_data)//1000)\n","  for i, doc in enumerate(tokenized_data):\n","    sum_emb = np.zeros(dim)\n","    for wrd in doc:\n","      curr_emb = word_embeddings.get_vector(wrd)\n","      res = cosine_similarity(avg_emb[i].reshape(1, -1), curr_emb.reshape(1, -1))\n","      sum_emb += avg_emb[i] * res[0][0]\n","    weighted_result += [sum_emb / len(doc)]\n","    if i%1000 == 0:\n","      print(\"|\", end=\"\", flush=True)\n","      \n","  return weighted_result\n","\n","## =================== SAVE DATA INTO FILE ==================== ##\n","def save_into_drive(data, filename):\n","  with open(filename, 'wb') as fid:\n","    pickle.dump(data, fid)\n","\n","def read_from_drive(filename):\n","  with open(filename, 'rb') as fid:\n","    return pickle.load(fid)"],"metadata":{"id":"e8-jFH0BwD_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## PREPROCESS DATA\n","train_sent_ids, train_sent_wds, train_word_count, train_word2id, train_id2word = data_preprocess(train_data['comment_tokenize'])\n","val_sent_ids, val_sent_wds, val_word_count, val_word2id, val_id2word = data_preprocess(valid_data['comment_tokenize'])\n","test_sent_ids, test_sent_wds, test_word_count, test_word2id, test_id2word = data_preprocess(test_data['comment_tokenize'])"],"metadata":{"id":"EURVmFB7A5cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## CHECK FOR EMPTY VALUES\n","print(\"Train: \", len(index_of_empty_tokenized_values(train_data)) == 0)\n","print(\"Valid: \", len(index_of_empty_tokenized_values(valid_data)) == 0)\n","print(\"Test: \", len(index_of_empty_tokenized_values(test_data)) == 0)\n","\n","## REMOVE EMPTY VALUES FROM DATASETS\n","train_data = remove_empty_tokenized_values(train_data) \n","valid_data = remove_empty_tokenized_values(valid_data) \n","test_data = remove_empty_tokenized_values(test_data) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7tb_PSlxPRQ","executionInfo":{"status":"ok","timestamp":1646513383884,"user_tz":480,"elapsed":443,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"27f65798-dc65-46ae-c014-bd4ec5467d1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train:  False\n","Valid:  False\n","Test:  False\n"]}]},{"cell_type":"code","source":["## WORD2VEC GENSIM\n","train_model = Word2Vec(train_sent_wds, min_count=1)\n","val_model = Word2Vec(val_sent_wds, min_count=1)\n","test_model = Word2Vec(test_sent_wds, min_count=1)"],"metadata":{"id":"Su_1JIO6HERk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## COMPUTE EMBEDDINGS\n","train_word_embeddings = train_model.wv\n","train_avg_emb = get_emb_avg(train_data['comment_tokenize'], train_word_embeddings, train_model.vector_size)\n","train_weighted_result = weighted_emb(train_data['comment_tokenize'], train_word_embeddings, train_avg_emb, train_model.vector_size)"],"metadata":{"id":"VTRrPxk_HdMS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646515170132,"user_tz":480,"elapsed":1630509,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"f15c7728-da81-416c-bf3b-151001532aed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["103\n","||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"]}]},{"cell_type":"code","source":["val_word_embeddings = val_model.wv\n","val_avg_emb = get_emb_avg(valid_data['comment_tokenize'], val_word_embeddings, val_model.vector_size)\n","val_weighted_result = weighted_emb(valid_data['comment_tokenize'], val_word_embeddings, val_avg_emb, val_model.vector_size)"],"metadata":{"id":"WFmwNBvIHriZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646516029348,"user_tz":480,"elapsed":859226,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"11848ef1-06b5-4c86-cb1a-c9a36a5ec080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["55\n","||||||||||||||||||||||||||||||||||||||||||||||||||||||||"]}]},{"cell_type":"code","source":["test_word_embeddings = test_model.wv\n","test_avg_emb = get_emb_avg(test_data['comment_tokenize'], test_word_embeddings, test_model.vector_size)\n","test_weighted_result = weighted_emb(test_data['comment_tokenize'], test_word_embeddings, test_avg_emb, test_model.vector_size)"],"metadata":{"id":"fq5SdVX8HvnB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646516975657,"user_tz":480,"elapsed":946318,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"a7af36d9-9f64-4873-ca12-ebe9ea9a7517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["63\n","||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"]}]},{"cell_type":"code","source":["## TRAIN MODEL\n","nb = GaussianNB()\n","nb.fit(train_weighted_result, train_data[\"toxic\"])"],"metadata":{"id":"V77ljrscHz6T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646516975819,"user_tz":480,"elapsed":170,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"51f4a179-168c-455a-cd5e-21d8f0d26b34"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB()"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["## SCORE ACCURACY (just to check that the datasets don't raise errors)\n","val_prob = nb.predict(val_weighted_result)\n","test_prob = nb.predict(test_weighted_result)\n","print(nb.score(val_weighted_result, valid_data[\"toxic\"]))\n","print(nb.score(test_weighted_result, test_data[\"toxic\"]))"],"metadata":{"id":"6ncJQvFBH3Ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646516976296,"user_tz":480,"elapsed":481,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"128d556a-6cba-45cb-f62c-d7774f090111"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6770131824043559\n","0.37076264517956625\n"]}]},{"cell_type":"code","source":["## GENERATE DATAFRAMES TO BE SAVED\n","train_emb = zip(train_weighted_result, train_data[\"toxic\"], train_data[\"severe_toxic\"], train_data[\"insult\"], train_data[\"obscene\"], train_data[\"threat\"], train_data[\"identity_hate\"])\n","valid_emb = zip(val_weighted_result, valid_data[\"toxic\"], valid_data[\"severe_toxic\"], valid_data[\"insult\"], valid_data[\"obscene\"], valid_data[\"threat\"], valid_data[\"identity_hate\"])\n","test_emb = zip(test_weighted_result, test_data[\"toxic\"], test_data[\"severe_toxic\"], test_data[\"insult\"], test_data[\"obscene\"], test_data[\"threat\"], test_data[\"identity_hate\"])"],"metadata":{"id":"zi-1pGpS0aQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## SAVE EMBEDDED DATA INTO FILES\n","save_into_drive(train_emb, 'train_emb.pkl')\n","save_into_drive(valid_emb, 'val_emb.pkl')\n","save_into_drive(test_emb, 'test_emb.pkl')"],"metadata":{"id":"nMwcHm72zWm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Appendix\n","### Compare Gensim model with our Skipgram implementation"],"metadata":{"id":"oVbC0QqFsBaR"}},{"cell_type":"code","source":["#GENERATE DATA FOR SKIPGRAM\n","\n","WINDOW_SIZE = 2 #smaller window size than Gensim\n","\n","data_skipgram = []\n","\n","for sent in train_sent_ids:\n","    for i in range(WINDOW_SIZE, len(sent) - WINDOW_SIZE):\n","        context = [sent[i - WINDOW_SIZE: i] + sent[i+1: i + WINDOW_SIZE + 1]]\n","        target  = sent[i]\n","        data_skipgram.append((context, target))\n","\n","print(\"Data_length:\",len(data_skipgram))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NaEIs9ZU-Pv","executionInfo":{"status":"ok","timestamp":1646354192810,"user_tz":480,"elapsed":17528,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"67372cad-b1e4-48bb-f6b3-64e048eb2047"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data_length: 3198009\n"]}]},{"cell_type":"code","source":["#OUR SKIPGRAM IMPLEMENTATION\n","\n","class SkipGram(nn.Module):\n","\n","    def __init__(self, vocab_size, hidden_size):\n","        super(SkipGram, self).__init__()\n","        self.u_emb = nn.Embedding(vocab_size, hidden_size)\n","        self.v_emb = nn.Embedding(vocab_size, hidden_size)\n","\n","    def forward(self, idx):\n","        return self.u_emb(idx)\n","\n","    def loss(self, pos_data, neg_data):\n","\n","        loss = 0\n","        targets = []\n","        all_pos_words = []\n","\n","        #GENERATING NEGATIVES\n","        all_neg_words = neg_data\n","     \n","        #GENERATING TARGET AND POSITIVES\n","        for id in range(len(pos_data)):\n","          targets.append(pos_data[id][1])\n","          for pid in range(len(pos_data[id][0][0])):\n","            all_pos_words.append(pos_data[id][0][0][pid])\n","\n","        targets = np.array(targets)\n","        all_pos_words = np.array(all_pos_words)\n","\n","        #GENERATING 3D TENSORS\n","        v_w = self.v_emb(torch.LongTensor(targets).to(device))\n","        u_w = self.u_emb(torch.LongTensor(all_pos_words).to(device))\n","        u_w_prime = self.u_emb(torch.LongTensor(all_neg_words).to(device))\n","\n","        v_w_pos = v_w.view(len(pos_data),v_w.shape[1],1)\n","        u_w_pos = u_w.view(len(pos_data),2*WINDOW_SIZE,u_w.shape[1])\n","        u_w_neg = u_w_prime.view(len(pos_data),10*2*WINDOW_SIZE,u_w_prime.shape[1])\n","        \n","        #LOSS POS + NEG\n","        pos_loss = torch.bmm(u_w_pos,v_w_pos).sigmoid().log()\n","        pos = pos_loss.sum(1) #sum over the 4 examples for each target word\n","        neg_loss = torch.bmm(u_w_neg.neg(),v_w_pos).sigmoid().log()\n","        neg = neg_loss.sum(1) #sum over the 40 examples for each target word\n","\n","        loss = -(pos+neg).mean()\n","\n","        return loss\n","  "],"metadata":{"id":"W6rZ7cr0Vcmt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DEFINE DEVICE\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"running on {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHz-xgEDVhzt","executionInfo":{"status":"ok","timestamp":1646354277446,"user_tz":480,"elapsed":7,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"20bef0c6-5442-4ec8-c544-81dc49983147"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["running on cuda\n"]}]},{"cell_type":"code","source":["#TRAINING OUR MODEL\n","MIN_COUNT = 1\n","\n","skipgram = SkipGram(len(train_word2id),100).to(device)\n","optimizer = optim.Adam(skipgram.parameters())\n","\n","vocabulary = {key: value for key, value in train_word_count.items() if value >= MIN_COUNT}\n","N = sum(vocabulary.values())\n","word_prob = {key: value/N for (key, value) in vocabulary.items()}\n","word_ID = list(word_prob.keys())\n","word_ID = [train_word2id[word] for word in word_ID]\n","\n","neg_sample_count = 10\n","itr_num = 30\n","batch_size = 2000\n","\n","l = []\n","for i in range(itr_num):\n","    print(\"iteration: \",i)\n","    s = 0\n","    for bid in range(len(data_skipgram) // batch_size): \n","        optimizer.zero_grad()\n","        positive_data = data_skipgram[bid * batch_size : (bid + 1) * batch_size]\n","        neg_data = []\n","\n","        word_prob_array = np.array(list(word_prob.values()))\n","        word_ID_array = np.array(word_ID)\n","\n","        neg_data = np.random.choice(word_ID_array,\n","                                    size=2*WINDOW_SIZE*neg_sample_count*batch_size,\n","                                    p=word_prob_array)\n","            \n","        loss = skipgram.loss(positive_data, neg_data)\n","        loss.backward()\n","        s += loss\n","        optimizer.step()\n","    l.append(s.item()/(len(data) // batch_size))\n","    print(\"Average Loss for the current iteration: \", l[i])\n","    print(\"-----------------------------------\")"],"metadata":{"id":"Wd5Njx9AVpAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#RESULTS FOR OUR IMPLEMENTATION\n","\n","word_embs  = skipgram.u_emb.weight.data\n","target_emb = skipgram.forward(torch.LongTensor([train_word2id['stupid']]).to(device))\n","# cosine similarity\n","similarity = (word_embs * target_emb).sum(dim=1) / (torch.norm(word_embs, dim=1) * torch.norm(target_emb))\n","for idx in similarity.argsort(descending=True).cpu().numpy()[1:11]:\n","    print(train_id2word[idx], similarity[idx].item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctVTUz7ZbHpE","executionInfo":{"status":"ok","timestamp":1646356640289,"user_tz":480,"elapsed":290,"user":{"displayName":"ANDREA CASASSA SIAN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05004144499949693318"}},"outputId":"1628a3df-038e-4a4d-d17b-51fe620dc88b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dumb 0.6962757110595703\n","stupid 0.6913631558418274\n","lick 0.6846228837966919\n","fat 0.6817508339881897\n","suck 0.6773995757102966\n","mom 0.6771062016487122\n","hell 0.6728000640869141\n","ass 0.6692366600036621\n","faggot 0.6688790917396545\n","banned 0.6661044359207153\n"]}]},{"cell_type":"code","source":["#RESULTS FOR GENSIM WORD2VECT MODEL\n","\n","#Min count set to 1 (we keep all the words)\n","model = Word2Vec(train_sent_wds, min_count=1) \n","\n","similar = model.wv.most_similar(\"stupid\")\n","for word in similar:\n","  print(word)"],"metadata":{"id":"necVmfz0svCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#WHICH MODEL TO USE?\n","\n","#We opted for Gensim --> more optimized"],"metadata":{"id":"l9CIBEkAbNF5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data analysis"],"metadata":{"id":"76MQaqMMwlE6"}},{"cell_type":"code","source":["#PERFORM SOME ANALYSIS ON SOME TOXIC WORDS \n","\n","#note: to be generalized to multiple words\n","\n","def word_count_in_comments(target, comment):\n","  count = 0\n","  for doc in comment:\n","    if target in doc:\n","      count += 1\n","    else:\n","      continue\n","\n","  contains_target = train_data.loc[train_data[\"comment\"].str.contains(target)]\n","  count_toxic = contains_target['toxic'].sum(0)\n","  count_severe_toxic = contains_target['severe_toxic'].sum(0)\n","  count_obscene = contains_target['obscene'].sum(0)\n","  count_threat = contains_target['threat'].sum(0)\n","  count_insult = contains_target['insult'].sum(0)\n","  count_identity_hate = contains_target['identity_hate'].sum(0)\n","\n","  print(\"Comments containing the target word: {}\" .format(count))\n","  print(\"Comments labeled as toxic: {}\" .format(count_toxic))\n","  print(\"Comments labeled as severe toxic: {}\" .format(count_severe_toxic))\n","  print(\"Comments labeled as obscene: {}\" .format(count_obscene))\n","  print(\"Comments labeled as threat: {}\" .format(count_threat))\n","  print(\"Comments labeled as insult: {}\" .format(count_insult))\n","  print(\"Comments labeled as identity hate: {}\" .format(count_identity_hate))"],"metadata":{"id":"7QO40-gswao3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w1 = 'stupid'\n","w2 = 'flower'\n","print(w1)\n","word_count_in_comments(w1, train_data['comment_tokenize'])\n","print(w2)\n","word_count_in_comments(w2, train_data['comment_tokenize'])"],"metadata":{"id":"U1jqCGBdwfE2"},"execution_count":null,"outputs":[]}]}